\title{\aux{Running a }\code{web} node}{concourse-web}{web-node}

\use-plugin{concourse-docs}

The \code{web} node is responsible for running the web UI, API, and as well
as performing all pipeline scheduling. It's basically the brain of Concourse.

\table-of-contents

\section{
  \title{Prerequisites}{web-prerequisites}

  Nothing special - the \code{web} node is a pretty simple Go application that
  can be run like a 12-factor app.
}

\section{
  \title{Running \code{concourse web}}{web-running}

  The \code{concourse} CLI can run as a \code{web} node via the \code{web}
  subcommand.

  Before running it, let's configure a local user so we can log in:

  \codeblock{bash}{{{
  CONCOURSE_ADD_LOCAL_USER=myuser:mypass
  CONCOURSE_MAIN_TEAM_LOCAL_USER=myuser
  }}}

  This will configure a single user, \code{myuser}, with the password
  \code{mypass}. You'll probably want to change those to sensible values, and
  later you may want to configure a proper auth provider - check out
  \reference{auth} whenever you're ready.

  Next, you'll need to configure the session signing key, the SSH key for the
  worker gateway, and the authorized worker key. Check
  \reference{generating-keys} to learn what these are and how they are created.

  \codeblock{bash}{{{
  CONCOURSE_SESSION_SIGNING_KEY=path/to/session_signing_key
  CONCOURSE_TSA_HOST_KEY=path/to/tsa_host_key
  CONCOURSE_TSA_AUTHORIZED_KEYS=path/to/authorized_worker_keys
  }}}

  Finally, \code{web} needs to know how to reach your Postgres database. This
  can be set like so:

  \codeblock{bash}{{{
  CONCOURSE_POSTGRES_HOST=127.0.0.1 # default
  CONCOURSE_POSTGRES_PORT=5432      # default
  CONCOURSE_POSTGRES_DATABASE=atc   # default
  CONCOURSE_POSTGRES_USER=my-user
  CONCOURSE_POSTGRES_PASSWORD=my-password
  }}}

  If you're running PostgreSQL locally, you can probably just point it to the
  socket and rely on the \code{peer} auth:

  \codeblock{bash}{{{
  CONCOURSE_POSTGRES_SOCKET=/var/run/postgresql
  }}}

  Now that everything's set, run:

  \codeblock{bash}{{{
  concourse web
  }}}

  All logs will be emitted to \code{stdout}, with any panics or lower-level
  errors being emitted to \code{stderr}.

  \section{
    \title{Configuring ingress traffic}{web-ingress}

    If your web nodes are going to be accessed over the network, you will need
    to set \code{CONCOURSE_EXTERNAL_URL} to a URL accessible by your Concourse
    users. If you don't set this property, logging in will incorrectly
    redirect to its default value of \code{127.0.0.1}.

    If your instance is available on the public internet, you may wish to
    prevent the Concourse UI from being nefariously embedded as an iframe by
    setting \code{CONCOURSE_X_FRAME_OPTIONS} to \code{deny} (to prevent
    \italic{any} iframe embeddings) or \code{sameorigin} (to only allow iframe
    embeddings in pages served from the same subdomain). This protects against
    clickjacking.

    \bold{Note:} If setting the value to \code{allow-from}, please note that not
    all browsers support this value and when not supported, the header is ignored
    by the browser.
  }

  \section{
    \title{Resource utilization}{web-resource-utilization}

    CPU usage: peaks during pipeline scheduling, primarily when scheduling
    \reference{jobs}. Mitigated by adding more \code{web} nodes. In this regard,
    \code{web} nodes can be considered compute-heavy more than anything else at
    large scale.

    Memory usage: not very well classified at the moment as it's not generally a
    concern. Give it a few gigabytes and keep an eye on it.

    Disk usage: none

    Bandwidth usage: aside from handling external traffic, the \code{web} node
    will at times have to stream bits out from one worker and into another while
    executing \reference{steps}.

    Highly available: yes; \code{web} nodes can all be configured the same (aside
    from \code{--peer-address}) and placed behind a load balancer. Periodic tasks
    like garbage-collection will not be duplicated for each node.

    Horizontally scalable: yes; they will coordinate workloads using the
    database, resulting in less work for each node and thus lower CPU usage.

    Outbound traffic:

    \list{
      \code{db} on its configured port for persistence
    }{
      \code{db} on its configured port for locking and coordinating in a
      multi-\code{web} node deployment
    }{
      other \code{web} nodes (possibly itself) on an ephemeral port when a worker
      is forwarded through the web node's TSA
    }

    Inbound traffic:

    \list{
      \code{worker} connects to the TSA on port \code{2222} for registration
    }{
      \code{worker} downloads inputs from the ATC during \reference{fly-execute}
      via its external URL
    }{
      external traffic to the ATC API via the web UI and \reference{fly-cli}
    }
  }
}

\section{
  \title{Operating a \code{web} node}{web-operation}

  The \code{web} nodes themselves are stateless - they don't store anything on
  disk, and coordinate entirely using the database.

  \section{
    \title{Scaling}

    The \reference{web-node} can be scaled up for high availability. They'll also
    roughly share their scheduling workloads, using the database to synchronize.
    This is done by just running more \code{web} commands on different machines,
    and optionally putting them behind a load balancer.

    To run a cluster of \reference{web-node}s, you'll first need to ensure
    they're all pointing to the same PostgreSQL server.

    Next, you'll need to configure a \italic{peer address}. This is a DNS or IP
    address that can be used to reach this \code{web} node from other
    \code{web} nodes. Typically this uses a private IP, like so:

    \codeblock{bash}{{{
    CONCOURSE_PEER_ADDRESS=10.10.0.1
    }}}

    This address will be used for forwarded worker connections, which listen on
    the ephemeral port range.

    Finally, if all of these nodes are going to be accessed through a load
    balancer, you'll need to configure the external URL that will be used to
    reach your Concourse cluster:

    \codeblock{bash}{{{
    CONCOURSE_EXTERNAL_URL=https://ci.example.com
    }}}

    Aside from the peer URL, all configuration must be consistent across all
    \code{web} nodes in the cluster to ensure consistent results.

    \section{
      \title{Database connection pooling}{web-connection-pooling}

      You may wish to configure the max number of parallel database connections
      that each node makes. There are two pools to configure: one for serving
      API requests, and one used for all the backend work such as pipeline
      scheduling.

      There are some non-configurable connection pools. They take up the
      following number of connections per pool:
      \list{
          Garbage Collection: 5
      }{
          Lock: 1
      }{
          Worker Registration: 1
      }

      The sum of these numbers across all \code{web} nodes should not be
      greater than the maximum number of simultaneous connections your Postgres
      server will allow. See \reference{db-resource-utilization}{\code{db} node
      resource utilization} for more information.

      For example, if 3 \code{web} nodes are configured as such:

      \codeblock{bash}{{{
      CONCOURSE_API_MAX_CONNS=10     # default
      CONCOURSE_BACKEND_MAX_CONNS=50 # default
      }}}

      ...then your PostgreSQL server should be configured with a connection
      limit of at least 201: (10 + 50 + 5 + 1 + 1) * 3.

    }
  }

  \section{
    \title{Reloading worker authorized key}

    While \reference{web-running}, the authorized worker key files are loaded at
    startup. During the lifecycle of a \reference{web-node} new \code{worker} keys
    might be added or old ones removed.

    The authorized key file containing \code{worker} public keys can be reloaded without
    downtime by sending \code{SIGHUP} to the \code{web} process. The process
    will remain running and Concourse will reload the keys to authorize.
  }

  \section{
    \title{Restarting & Upgrading}

    The \code{web} nodes can be killed and restarted willy-nilly. No draining
    is necessary; if the \code{web} node was orchestrating a build it will just
    continue where it left off when it comes back, or the build will be
    picked up by one of the other \code{web} nodes.

    To upgrade a \code{web} node, stop its process and start a new one using
    the newly installed \code{concourse}. Any migrations will be run
    automatically on start. If \code{web} nodes are started in parallel, only
    one will run the migrations.

    Note that we don't currently guarantee a lack of funny-business if you're
    running mixed Concourse versions - database migrations can perform
    modifications that confuse other \code{web} nodes. So there may be some
    turbulence during a rolling upgrade, but everything should stabilize once
    all \code{web} nodes are running the latest version.
  }

  \section{
    \title{Downgrading}

    If you're stuck in a pinch and need to downgrade from one version of
    Concourse to another, you can use the \code{concourse migrate} command.

    \italic{
      Note: support for down migrations is a fairly recent addition to
      Concourse; it is not supported for downgrading to v3.6.0 and below.
    }

    First, grab the desired migration version by running the following:

    \codeblock{bash}{{{
    # make sure this is the *old* Concourse binary
    $ concourse migrate --supported-db-version
    1551110547
    }}}

    That number (yours will be different) is the expected migration version for
    that version of Concourse.

    Next, run the following with the \italic{new} Concourse binary:

    \codeblock{bash}{{{
    $ concourse migrate --migrate-db-to-version=1551110547
    }}}

    This will need the same \code{CONCOURSE_POSTGRES_*} configuration described
    in \reference{web-running}.

    Once this completes, switch all \code{web} nodes back to the older
    \code{concourse} binary and you should be good to go.
  }
}

\section{
  \title{Configuring the \code{web} node}{web-configuration}

  \section{
    \title{Giving your cluster a name}

    If you've got many Concourse clusters that you switch between, you can make
    it slightly easier to notice which one you're on by giving each cluster a
    name:

    \codeblock{bash}{{{
    CONCOURSE_CLUSTER_NAME=production
    }}}

    When set, this name will be shown in the top bar when viewing the dashboard.
  }

  \section{
    \title{TLS via Let's Encrypt}{lets-encrypt}

    Concourse can be configured to automatically acquire a TLS certificate via
    \link{Let's Encrypt}{https://letsencrypt.org}:

    \codeblock{bash}{{{
      # Enable TLS
      CONCOURSE_TLS_BIND_PORT=443

      # Enable Let's Encrypt
      CONCOURSE_ENABLE_LETS_ENCRYPT=true
    }}}

    \warn{
      Concourse's Let's Encrypt integration works by storing the TLS
      certificate and key in the database, so it is imperative that you enable
      \reference{encryption}{database encryption} as well.
    }

    By default, Concourse will reach out to \link{Let's Encrypt's ACME CA
    directory}{https://acme-v02.api.letsencrypt.org/directory}. An alernative
    URL can be configured like so:

    \codeblock{bash}{{{
      CONCOURSE_LETS_ENCRYPT_ACME_URL=https://acme.example.com/directory
    }}}

    In order to negotiate the certificate, your \code{web} node must be
    reachable by the ACME server. There are intentionally \link{no publicly
    listed IP addresses to whitelist}{https://letsencrypt.org/docs/faq/}, so
    this typically means just making your \code{web} node publicly reachable.
  }

  \section{
    \title{Enabling audit logs}{audit-logs}

    A very simplistic form of audit logging can be enabled with the following
    vars:

    \codeblock{bash}{{{
      # Enable auditing for all api requests connected to builds.
      CONCOURSE_ENABLE_BUILD_AUDITING=true

      # Enable auditing for all api requests connected to containers.
      CONCOURSE_ENABLE_CONTAINER_AUDITING=true

      # Enable auditing for all api requests connected to jobs.
      CONCOURSE_ENABLE_JOB_AUDITING=true

      # Enable auditing for all api requests connected to pipelines.
      CONCOURSE_ENABLE_PIPELINE_AUDITING=true

      # Enable auditing for all api requests connected to resources.
      CONCOURSE_ENABLE_RESOURCE_AUDITING=true

      # Enable auditing for all api requests connected to system transactions.
      CONCOURSE_ENABLE_SYSTEM_AUDITING=true

      # Enable auditing for all api requests connected to teams.
      CONCOURSE_ENABLE_TEAM_AUDITING=true

      # Enable auditing for all api requests connected to workers.
      CONCOURSE_ENABLE_WORKER_AUDITING=true

      # Enable auditing for all api requests connected to volumes.
      CONCOURSE_ENABLE_VOLUME_AUDITING=true
    }}}

    When enabled, API requests will result in an info-level log line like so:

    \codeblock{json}{{{
    {"timestamp":"2019-05-09T14:41:54.880381537Z","level":"info","source":"atc","message":"atc.audit","data":{"action":"Info","ip":"192.168.2.55","parameters":{},"user":"test"}}
    {"timestamp":"2019-05-09T14:42:36.704864093Z","level":"info","source":"atc","message":"atc.audit","data":{"action":"GetPipeline","ip":"192.168.2.55","parameters":{":pipeline_name":["booklit"],":team_name":["main"]},"user":"test"}}
    }}}

    A user's IP address is logged based on available headers in the request. The auditor will search, in this order, for:
    \list{
      The custom header set by \code{CONCOURSE_CLIENT_IP_HEADER}
      }{
      \code{X-REAL-IP}
      }{
      \code{X-FORWARDED-FOR}
      }{
      \code{RemoteAddr} in the request.
      }
  }

  \section{
    \title{Configuring defaults for resource types}{resource-defaults}

    Defaults for the "core" resource types (\link{those that show up under the Concourse
    org}{https://github.com/concourse?q=-resource})
    that comes with Concourse can be set cluster-wide by passing in a configuration file.
    The format of the file is the name of the resource type followed by an arbitrary
    configuration.

    Documentation for each resource type's configuration is
    in each implementation's \code{README}.

    \codeblock{bash}{{{
    CONCOURSE_BASE_RESOURCE_TYPE_DEFAULTS=./defaults.yml
    }}}

    For example, a \code{defaults.yml} that configures the entire cluster to use a
    registry mirror would have:
    \codeblock{yaml}{{{
    registry-image:
      registry_mirror:
        host: https://registry.mirror.example.com
    }}}
  }
}
